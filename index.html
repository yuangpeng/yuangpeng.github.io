<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Yuang Peng</title> <meta name="author" content="Yuang Peng"> <meta name="description" content="The personal website of Yuang Peng. "> <meta name="keywords" content="Yuang Peng, AI, artificial intelligence, generative model, large language model"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/llama.png?5863191f325e6969d44e8febecd83fed"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuangpeng.com/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <article> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/llama43-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/llama43-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/llama43-1400.webp"></source> <img src="/assets/img/llama43.png" class="img-fluid z-depth-0 rounded" width="auto" height="auto" alt="llama43.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-9"> <h1 class="post-title"> Yuang Peng (彭雨昂) </h1> <p class="desc"><strong>Master Student</strong> at ITML Group, <a href="https://www.tsinghua.edu.cn/" rel="external nofollow noopener" target="_blank">Tsinghua University</a>. <br><br> <strong>Research Intern</strong>, <br> Foundation Model Group, <a href="https://megvii.com/megvii_research" rel="external nofollow noopener" target="_blank">Megvii Research (Face++)</a>. </p> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%75%61%6E%67%70%65%6E%67.%63%6E@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=J0ko04IAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2211415443" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/yuangpeng" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://twitter.com/yuang_peng" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> </div> </div> </div> <div class="clearfix"> <p><strong>Research:</strong> As a researcher and engineer specializing in large language model, my primary focus on the development of efficient and scalable methods for multimodal data modeling, with particular emphasis on text, images, and videos. My interest spans multiple ares, including generative modeling, representation learning, reinforcement learning, and embodied AI. My ultimate ambition is to cultivate multimodal perception, reasoning, and generation capabilities for Artificial General Intelligence (AGI), with the goal of creating fully intelligent systems and robots that can enhance human lives.</p> <p><strong>Experience:</strong> I am currently pursuing my Master’s degree in Computer Science at <a href="https://www.tsinghua.edu.cn/" rel="external nofollow noopener" target="_blank">Tsinghua University</a>, advised by <a href="https://scholar.google.com/citations?user=koAXTXgAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">Shutao Xia</a> and <a href="https://scholar.google.com.hk/citations?user=Yl0wv7AAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">Bin Chen</a>. I was a research intern at Foundation Model Group, <a href="https://megvii.com/megvii_research" rel="external nofollow noopener" target="_blank">Megvii Research</a>, and <a href="https://www.shlab.org.cn" rel="external nofollow noopener" target="_blank">Shanghai Artificial Intelligence Laboratory</a>. I was a short-term visiting scholar at the Artificial Intelligence Group, <a href="https://www.cam.ac.uk" rel="external nofollow noopener" target="_blank">University of Cambridge</a>, advised by <a href="https://scholar.google.com/citations?user=4YhNJBEAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Pietro Liò</a>. I obtained my Bachelor’s degree in Computer Science from <a href="https://www.whu.edu.cn/" rel="external nofollow noopener" target="_blank">Wuhan University</a>, where I was recognized as a distinguished graduate and graduated <em>summa cum laude</em>.</p> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Sep 20, 2023</th> <td> Introduce multimodal LLM: <a href="https://dreamllm.github.io" rel="external nofollow noopener" target="_blank">DreamLLM</a> </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div> <div id="dong2023dreamllm" class="col-sm-9"> <div class="title">Dreamllm: Synergistic multimodal comprehension and creation</div> <div class="author"> <a href="https://scholar.google.com/citations?user=z2SoXI8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Runpei Dong*</a>, Chunrui Han*, <em>Yuang Peng</em>, Zekun Qi, <a href="https://scholar.google.com/citations?user=hJ-VrrIAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Zheng Ge</a>, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Xiangyu Zhang</a>, Kaisheng Ma, and Yi Li</div> <div class="periodical"> <em>arXiv preprint arXiv:2309.11499</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2309.11499" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM’s superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div> <div id="zhao2023chatspot" class="col-sm-9"> <div class="title">Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning</div> <div class="author"> Liang Zhao*, En Yu*, <a href="https://scholar.google.com/citations?user=hJ-VrrIAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Zheng Ge</a>, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, <em>Yuang Peng</em>, <a href="https://scholar.google.com/citations?user=z2SoXI8AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Runpei Dong</a>, Chunrui Han, and <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Xiangyu Zhang</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2307.09474</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2307.09474" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot’s promising performance.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Yuang Peng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Learned from <a href="https://yang-song.net" rel="external nofollow noopener" target="_blank">Yang Song</a>. Last updated: October 31, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1PXPLJH1FH"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1PXPLJH1FH");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>